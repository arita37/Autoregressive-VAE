{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from autoencoder_linear import LayerNorm, FeedForward, clones, Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model, N, head_num, d_ff = 10, 3, 2, 20  # N: number of layer, head_num: number of head\n",
    "encoder = Encoder(d_model, N, head_num, d_ff)\n",
    "decoder = Decoder(d_model, N, head_num, d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len = 2, 64\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "mask = torch.ones(batch_size, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- encoder input: torch.Size([2, 64, 10])\n",
      "- encoder: torch.Size([2, 64, 10])\n",
      "- encoder: torch.Size([2, 64, 10])\n",
      "- encoder: torch.Size([2, 64, 10])\n",
      "- encoder: torch.Size([2, 32, 10])\n",
      "- encoder: torch.Size([2, 16, 10])\n",
      "- encoder: torch.Size([2, 8, 10])\n"
     ]
    }
   ],
   "source": [
    "memory = encoder(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- decoder input: torch.Size([2, 8, 10])\n",
      "- decoder: torch.Size([2, 16, 10])\n",
      "- decoder: torch.Size([2, 32, 10])\n",
      "- decoder: torch.Size([2, 64, 10])\n",
      "- decoder: torch.Size([2, 64, 10])\n",
      "- decoder: torch.Size([2, 64, 10])\n",
      "- decoder: torch.Size([2, 64, 10])\n"
     ]
    }
   ],
   "source": [
    "output = decoder(memory, torch.ones(batch_size, 1, memory.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[ 1.3956,  1.1497, -1.3392,  0.8979,  1.0989],\n",
    "                       [-0.3986,  0.0929, -0.2454,  2.4595,  0.3489],\n",
    "                       [ 0.4532,  0.6627,  0.6888, -0.9131, -1.8882]], requires_grad=True)\n",
    "targets = torch.tensor([1, 0, 4])\n",
    "print(\"Inputs: {}\".format(inputs.shape))\n",
    "print(\"Targets: {}\".format(targets.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = torch.nn.functional.log_softmax(inputs, dim=-1)\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = criterion(ls, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor([[ 0.1042, -0.2518,  0.0068,  0.0634,  0.0775],\n",
    "#         [-0.3191,  0.0233,  0.0166,  0.2489,  0.0302],\n",
    "#         [ 0.0866,  0.1068,  0.1096,  0.0221, -0.3250]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[[10, 1, 1],\n",
    "                        [1, 10, 1],\n",
    "                        [1, 1, 10]]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, preds = torch.max(log_probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([[1, 2, 3, 4],\n",
    "                       [2, 3, 4, 5]], dtype=torch.int32)\n",
    "targets = torch.tensor([[1, 2, 3, 0],\n",
    "                        [2, 3, 4, 0]], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(labels == targets).item() / torch.sum(torch.ones_like(labels)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels == targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean((labels == targets).float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view(1, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_: [embedding_dim, num_embeddings]\n",
    "# flat_inputs: [N, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ = torch.tensor([[1., 2.],\n",
    "                   [2., 3.],\n",
    "                   [3., 4.]])\n",
    "flat_inputs = torch.tensor([[2. , 3., 5.],\n",
    "                            [1. , 2., 3.],\n",
    "                            [2. , 4., 6.],\n",
    "                            [2. , 4., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38.],\n",
       "        [14.],\n",
       "        [56.],\n",
       "        [56.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(flat_inputs**2).sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 29.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w_**2).sum(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[46., 66.],\n",
       "        [28., 40.],\n",
       "        [56., 80.],\n",
       "        [56., 80.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * torch.matmul(flat_inputs, w_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = (flat_inputs**2).sum(dim=1, keepdim=True) - 2 * torch.matmul(flat_inputs, w_) + (w_**2).sum(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  1.],\n",
       "        [ 0.,  3.],\n",
       "        [14.,  5.],\n",
       "        [14.,  5.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances # [N, num_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_indices = torch.argmax(-distances, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = torch.zeros(encoding_indices.shape[0],\n",
    "                                  2).scatter_(1, encoding_indices.unsqueeze(1), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(encoding_indices, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can only calculate the mean of floating types. Got Long instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-3ade2513c2b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Can only calculate the mean of floating types. Got Long instead."
     ]
    }
   ],
   "source": [
    "torch.mean(torch.nn.functional.one_hot(encoding_indices, 2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-87d83e2d96b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m distances = (tf.reduce_sum(flat_inputs**2, 1, keepdims=True)\n\u001b[0;32m      2\u001b[0m              \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m              + tf.reduce_sum(w ** 2, 0, keepdims=True))\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "distances = (tf.reduce_sum(flat_inputs**2, 1, keepdims=True)\n",
    "             - 2 * tf.matmul(flat_inputs, w)\n",
    "             + tf.reduce_sum(w ** 2, 0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, dim_embedding, num_embeddings, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self._commitment_cost = commitment_cost\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings, dim_embedding)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: [..., dim_embedding]\n",
    "        \"\"\"\n",
    "        assert inputs.shape[-1] == self.dim_embedding\n",
    "        flat_inputs = inputs.view(-1, inputs.shape[-1])\n",
    "        \n",
    "        # distance\n",
    "        w = self.embed.weight.detach()\n",
    "        with torch.no_grad():\n",
    "            distances = ((flat_inputs**2).sum(dim=1, keepdim=True)\n",
    "                         - 2 * torch.matmul(flat_inputs, w.T)  # distances: [N, num_embeddings]\n",
    "                         + ((w.T)**2).sum(dim=0, keepdim=True))\n",
    "            encoding_indices = torch.argmax(-distances, 1)  # [N]\n",
    "            encodings = F.one_hot(encoding_indices, self.num_embeddings)\n",
    "            encoding_indices = encoding_indices.view(inputs.shape[:-1])\n",
    "        \n",
    "        # get quantized vectors\n",
    "        quantized = self.embed(encoding_indices)\n",
    "        \n",
    "        # compute loss\n",
    "        q_latent_loss = torch.mean((inputs.detach() - quantized) ** 2)\n",
    "        e_latent_loss = torch.mean((inputs - quantized.detach()) ** 2)\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized.detach() - inputs.detach())\n",
    "\n",
    "        return {'quantize': quantized,\n",
    "                'loss': loss,\n",
    "                'encodings': encodings,\n",
    "                'encoding_indices': encoding_indices,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantizer(3, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(2, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = vq(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['quantize'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
